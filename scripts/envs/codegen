# all default common env var values are defined in scripts/envs/base file
# in order to override a default common env var value and/or add a custom
# env var used only for this sample you can list it here
export APP_NAME="codegen"
export APP_DISPLAY_NAME="Code Generation Application"
export APP_DESC="Build your own Large language model (LLM)-enabled code generation application to generate code source from a natural language. Pick from the model servers available or bring your own."
export APP_TAGS='["ai", "llamacpp", "vllm", "python"]'
export APP_RUN_COMMAND="streamlit run codegen-app.py"

# https://github.com/containers/ai-lab-recipes/tree/main/model_servers/llamacpp_python
export MODEL_SERVICE_CONTAINER="quay.io/ai-lab/llamacpp_python:latest"
export MODEL_SERVICE_CONTAINER_DESC="This Python binding of llama.cpp provides LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud."

# https://github.com/rh-aiservices-bu/llm-on-openshift/tree/main/llm-servers/vllm/gpu
export VLLM_CONTAINER="quay.io/rh-aiservices-bu/vllm-openai-ubi9:0.4.2"
export VLLM_CONTAINER_DESC="OpenShift deployment of vLLM is a fast and easy-to-use library for LLM inference and serving."

# https://huggingface.co/Nondzu/Mistral-7B-code-16k-qlora
# https://github.com/redhat-ai-dev/developer-images/tree/main/model/mistral-7b-code-16k-qlora
export INIT_CONTAINER="quay.io/redhat-ai-dev/mistral-7b-code-16k-qlora:latest"
export VLLM_MODEL_NAME="Nondzu/Mistral-7B-code-16k-qlora"
export VLLM_MAX_MODEL_LEN=6144
export VLLM_MODEL_DESC="Small and fast model for supporting coding or acting as a copilot."

# model configurations
export SUPPORT_LLM=true

# for gitlab case, since gitlab does not have pipeline webhook pre-set to trigger the initial build
export APP_INTERFACE_CONTAINER="quay.io/redhat-ai-dev/codegen:latest"
