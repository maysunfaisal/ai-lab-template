export APP_NAME="rag"
export APP_DISPLAY_NAME="RAG Chatbot Application"
export APP_DESC="RAG Chatbot Application example with an LLM and a Vector database enabled"
export APP_TAGS='["ai", "llamacpp", "vllm", "python", "rag", "database"]'
export APP_RUN_COMMAND="streamlit run rag_app.py"
export INIT_CONTAINER="quay.io/redhat-ai-dev/granite-7b-lab:latest"
export INIT_CONTAINER_COMMAND="['/usr/bin/install', '/model/model.file', '/shared/']"
export APP_PORT=8501
export MODEL_SERVICE_CONTAINER="quay.io/ai-lab/llamacpp_python:latest"
export MODEL_SERVICE_PORT=8001

# model configurations
export SUPPORT_LLM=true
export SUPPORT_ASR=false
export SUPPORT_DETR=false

export VLLM_CONTAINER="quay.io/rh-aiservices-bu/vllm-openai-ubi9:0.4.2"
export VLLM_MODEL_NAME="instructlab/granite-7b-lab"
export VLLM_MAX_MODEL_LEN=4096

# for gitlab case, since gitlab does not have pipeline webhook pre-set to trigger the initial build
export APP_INTERFACE_CONTAINER="quay.io/redhat-ai-dev/rag:latest"

# Database Required
export SUPPORT_DB=true
export DB_CONTAINER="quay.io/redhat-ai-dev/chroma:latest"
export DB_PORT=8000
